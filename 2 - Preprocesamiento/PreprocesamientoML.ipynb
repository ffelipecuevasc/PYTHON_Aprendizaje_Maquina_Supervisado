{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# üìå Proyecto de Preprocesamiento\n",
    "\n",
    "Este proyecto se divide en 2 etapas:\n",
    "**- Etapa 1** = Pasos 1 al 4 donde se aprende de manera manual a realizar el preprocesamiento (carga de datos, limpieza, codificaci√≥n, escalamiento) paso a paso utilizando Pandas, Numpy.\n",
    "**- Etapa 2** = Pasos 5 al 7 donde se realiza el preprocesamiento de manera automatizada con pipelines, empaquetando todo lo realizado en los pasos 1 - 4 con SciKit Learn."
   ],
   "id": "8253b27a4998f23f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Paso 1:** Conexi√≥n a MongoDB y carga de datos\n",
    "\n",
    "Ac√° realizamos la conexi√≥n a la base de datos en MongoDB y cargamos los datos en un Data Frame de Pandas."
   ],
   "id": "e9f9e645fe81161f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Conectarse al servidor local de MongoDB (ajustar si usas otro host o puerto)\n",
    "conexion = MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# Seleccionar base de datos y colecci√≥n\n",
    "bd = conexion[\"becas_chile\"]\n",
    "coleccion = bd[\"postulantes_becas\"]\n",
    "\n",
    "# Obtener los datos y convertirlos a DataFrame\n",
    "# .find(): ejecuta una consulta a MongoDB, equivalente a un \"SELECT *\" en SQL.\n",
    "# - Primer argumento {}: es el filtro de b√∫squeda. En este caso est√° vac√≠o ({}), lo que significa que selecciona todos los documentos sin filtrar nada.\n",
    "datos = list(coleccion.find({}))\n",
    "df = pd.DataFrame(datos)\n",
    "df = df.drop(columns=[\"_id\"])\n",
    "\n",
    "# Mostrar primeras filas del DataFrame\n",
    "df.head()"
   ],
   "id": "d8d06e8b64cbf24a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tama√±o del dataset\n",
    "print(f\"Cantidad de registros: {df.shape[0]}\")\n",
    "print(f\"Cantidad de columnas: {df.shape[1]}\")"
   ],
   "id": "94132204d8ca1cb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Tipos de datos\n",
    "print(\"\\nTipos de datos:\")\n",
    "print(df.dtypes)"
   ],
   "id": "71f08bce89ea9e7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Valores nulos\n",
    "print(\"\\nValores nulos por columna:\")\n",
    "print(df.isnull().sum())"
   ],
   "id": "1bc458cf5ee44bb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Paso 2:** Limpieza y tratamiento general\n",
    "\n",
    "En este paso revisaremos la calidad de los datos:\n",
    "- Verificaremos si hay registros duplicados\n",
    "- Trataremos valores nulos\n",
    "- Exploraremos posibles valores at√≠picos (outliers)"
   ],
   "id": "e7efd92eb0da3418"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verificamos si hay duplicados exactos\n",
    "duplicados = df.duplicated().sum()\n",
    "print(f\"Registros duplicados encontrados: {duplicados}\")\n",
    "\n",
    "# Si hay, los eliminamos\n",
    "df = df.drop_duplicates()"
   ],
   "id": "5682773ec1c2a260",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Volvemos a revisar si hay valores nulos por columna\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# En este dataset simulado, no deber√≠an haber nulos. Pero si los hubiera:\n",
    "# df['ingreso_familiar'] = df['ingreso_familiar'].fillna(df['ingreso_familiar'].median())\n",
    "# df['promedio_notas'] = df['promedio_notas'].fillna(df['promedio_notas'].mean())"
   ],
   "id": "4fcace95dee35940",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Podemos ver algunos valores extremos visualmente\n",
    "# Aqu√≠ buscamos valores muy bajos o muy altos, por ejemplo ingresos fuera de rango o notas fuera del sistema chileno (1.0 - 7.0).\n",
    "df.describe()"
   ],
   "id": "b6d5c1b28d1f1a00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Paso 3:** Codificaci√≥n de variables categ√≥ricas\n",
    "\n",
    "Los modelos de Machine Learning trabajan con n√∫meros, no con texto.\n",
    "Por eso, debemos transformar las variables categ√≥ricas en variables num√©ricas.\n",
    "\n",
    "En este dataset tenemos principalmente tres variables categ√≥ricas:\n",
    "- `sexo` ‚Üí Binaria (M / F)\n",
    "- `tipo_colegio` ‚Üí Nominal (municipal / subvencionado / particular)\n",
    "- `comuna` ‚Üí Nominal con m√∫ltiples categor√≠as\n",
    "\n",
    "Para codificarlas usaremos:\n",
    "- Variables dummy (binarias) para `sexo` y `tipo_colegio`\n",
    "- One-Hot Encoding para `comuna`"
   ],
   "id": "fa75f74782130eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Seleccionamos las columnas categ√≥ricas (tipo object o string)\n",
    "columnas_categoricas = df.select_dtypes(include=[\"object\"]).columns\n",
    "print(\"Columnas categ√≥ricas detectadas:\", list(columnas_categoricas))"
   ],
   "id": "d8ad5c1e00aa4b32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Creamos una columna binaria: M = 1, F = 0\n",
    "df[\"sexo\"] = df[\"sexo\"].map({\"M\": 1, \"F\": 0})\n",
    "df.head()"
   ],
   "id": "9459a4aa433207f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Codificar tipo_colegio y comuna usando One-Hot Encoding\n",
    "df = pd.get_dummies(df, columns=[\"tipo_colegio\", \"comuna\"], drop_first=True)\n",
    "df.head()"
   ],
   "id": "d5b55ac3610622d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ahora el DataFrame solo tiene variables num√©ricas, listas para el paso 4 (Escalamiento de variables num√©ricas).\n",
    "\n",
    "Ventajas:\n",
    "- Los modelos de ML ya pueden interpretar la informaci√≥n\n",
    "- Evitamos errores por categor√≠as de texto\n",
    "- El escalamiento en el siguiente paso solo se aplicar√° a las variables num√©ricas que lo necesiten\n",
    "\n",
    "Pero antes:\n",
    "- Verifiquemos que tras la codificaci√≥n todas las columnas son num√©ricas y no qued√≥ ninguna categ√≥rica sin transformar.\n",
    "- Todas las columnas deber√≠an ser int64 o float64.\n",
    "- No deber√≠a quedar ninguna columna tipo object (texto).\n",
    "- Los nombres de columnas nuevas seguir√°n el formato tipo_colegio_xxx o comuna_xxx."
   ],
   "id": "dc6393a80bf393d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Lista final de columnas despu√©s de la codificaci√≥n\n",
    "print(\"Columnas finales del DataFrame:\")\n",
    "print(list(df.columns))"
   ],
   "id": "5061dad9813a092d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verificar tipos de datos\n",
    "print(\"\\nTipos de datos despu√©s de la codificaci√≥n:\")\n",
    "print(df.dtypes.value_counts())"
   ],
   "id": "728f3482a3b96263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Mostrar primeras filas para inspecci√≥n\n",
    "df.head()"
   ],
   "id": "20bec364def1a981",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Paso 4:** Escalamiento de variables num√©ricas\n",
    "\n",
    "El escalamiento permite que todas las variables num√©ricas est√©n en un rango comparable.\n",
    "\n",
    "En este caso aplicaremos:\n",
    "- **Min-Max Scaling** para llevar los valores a un rango entre 0 y 1.\n",
    "- **Estandarizaci√≥n (Z-Score)** para tener media 0 y desviaci√≥n est√°ndar 1.\n",
    "\n",
    "Seg√∫n el algoritmo de ML que usemos, elegiremos una u otra.\n",
    "Por ejemplo:\n",
    "- KNN, K-Means ‚Üí necesitan escalamiento\n",
    "- Modelos basados en √°rboles (Random Forest, XGBoost) ‚Üí no lo necesitan"
   ],
   "id": "a5ae4782ea524e71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identificar columnas num√©ricas a escalar\n",
    "\n",
    "# Seleccionar columnas num√©ricas (excluyendo la variable objetivo si existe)\n",
    "columnas_numericas = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "print(\"Columnas num√©ricas detectadas:\", list(columnas_numericas))"
   ],
   "id": "bc674c1877c3963e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Aplicar Min-Max Scaling\n",
    "\n",
    "# Creamos una copia para no sobreescribir df original\n",
    "df_minmax = df.copy()\n",
    "\n",
    "# Escalador Min-Max\n",
    "minmax_scaler = MinMaxScaler()\n",
    "df_minmax[columnas_numericas] = minmax_scaler.fit_transform(df_minmax[columnas_numericas])\n",
    "\n",
    "print(\"DataFrame con Min-Max Scaling:\")\n",
    "df_minmax.head()"
   ],
   "id": "e718b746ac1ce9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Aplicar Estandarizaci√≥n (Z-Score)\n",
    "\n",
    "# Creamos una copia para no sobreescribir df original\n",
    "df_standard = df.copy()\n",
    "\n",
    "# Escalador Standard\n",
    "standard_scaler = StandardScaler()\n",
    "df_standard[columnas_numericas] = standard_scaler.fit_transform(df_standard[columnas_numericas])\n",
    "\n",
    "print(\"DataFrame con Estandarizaci√≥n (Z-Score):\")\n",
    "df_standard.head()"
   ],
   "id": "155b26c63ac0aee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Solo escalamos **variables num√©ricas**.\n",
    "- Variables dummy o One-Hot Encoding **no deben escalarse** porque perder√≠an su sentido.\n",
    "- El escalador se debe ajustar (`fit`) **solo con los datos de entrenamiento** y luego aplicar (`transform`) a entrenamiento y prueba para evitar *data leakage*.\n",
    "\n",
    "Para aprender **¬øqu√© pas√≥ con la columna edad?**"
   ],
   "id": "783c11b7d02c1526"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Paso 5:** Preprocesamiento de datos enfocados en Machine Learning\n",
    "\n",
    "Ac√° vamos a encapsular TODAS las transformaciones de datos (imputaci√≥n, codificaci√≥n, escalamiento) en un flujo reproducible y seguro contra *data leakage*.\n",
    "\n",
    "**Idea clave:** dise√±amos un `preprocessor` que:\n",
    "- Para columnas **num√©ricas**: imputa faltantes con MEDIANA y estandariza (media=0, std=1).\n",
    "- Para columnas **categ√≥ricas**: imputa faltantes con la categor√≠a m√°s frecuente y hace One-Hot Encoding (eliminando la primera categor√≠a para evitar multicolinealidad en modelos lineales).\n",
    "\n",
    "**Importante:** en este paso **NO ajustamos** (`fit`) el pipeline. El ajuste ocurre en el **Paso 6**, *solo con el set de entrenamiento*."
   ],
   "id": "e5c6004da4ac6119"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Librer√≠as necesarias para construir un pipeline\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ],
   "id": "63734665fbb0e638",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Importaci√≥n de datos nuevamente para trabajar con ellos en crudo\n",
    "cliente = MongoClient(\"mongodb://localhost:27017/\")\n",
    "basedatos = cliente[\"becas_chile\"]\n",
    "coleccion = basedatos[\"postulantes_becas\"]\n",
    "\n",
    "datos_raw = list(coleccion.find({}))\n",
    "df_raw = pd.DataFrame(datos_raw)\n",
    "df_raw = df_raw.drop(columns=[\"_id\"])\n",
    "df_raw.head()"
   ],
   "id": "de05a24fb5e0bf5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Declarar contrato en las columnas\n",
    "# Separar la variable objetivo (y): postula_a_beca (Booleano)\n",
    "# Separar la variable independiente (X): campos num√©ricos y categ√≥ricos expl√≠citos.\n",
    "\n",
    "# Campo objetivo = y\n",
    "target = \"postula_a_beca\"\n",
    "\n",
    "cols_numericas = [\"edad\", \"puntaje_paes\", \"promedio_notas\", \"ingreso_familiar\"]\n",
    "cols_categoricas = [\"sexo\", \"comuna\", \"tipo_colegio\"]\n",
    "\n",
    "X_raw = df_raw[cols_numericas + cols_categoricas].copy()\n",
    "y = df_raw[target].copy()"
   ],
   "id": "217405d24e104fd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dise√±ar los pipelines que usaremos\n",
    "\n",
    "**Num√©ricas**\n",
    "- `SimpleImputer(median)` ‚Üí robusto a outliers\n",
    "- `StandardScaler()` ‚Üí media 0 y desv√≠o 1\n",
    "\n",
    "**Categ√≥ricas**\n",
    "- `SimpleImputer(most_frequent)` ‚Üí relleno consistente\n",
    "- `OneHotEncoder(handle_unknown='ignore', drop='first')` ‚Üí robusto a categor√≠as nuevas y evita colinealidad\n"
   ],
   "id": "9aeb11f6f269786c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\"))\n",
    "])"
   ],
   "id": "83245e98a4c1018b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preprocessor - Resposable de transformaci√≥n\n",
    "\n",
    "El preprocessor ac√° es como la ‚Äúm√°quina de preparaci√≥n de datos‚Äù que encapsula todas las reglas que nuestro dataset necesita para estar en un formato ideal antes de entrar a un modelo de Machine Learning.\n",
    "\n",
    "No es un modelo predictivo, sino un transformador de datos, pero su papel es tan cr√≠tico que, si no lo tienes bien definido y controlado, el rendimiento y la reproducibilidad del modelo se pueden ir al suelo.\n",
    "\n",
    "### Por qu√© es **necesario** para los pasos 6 y 7\n",
    "**- Paso 6:** Aqu√≠ ajustamos (fit) el preprocessor solo con entrenamiento y aplicamos las mismas reglas al test. Sin preprocessor, tendr√≠as que reescribir manualmente cada imputaci√≥n, escalado y codificaci√≥n.\n",
    "\n",
    "**- Paso 7:** El modelo espera datos ya transformados y con el mismo n√∫mero/orden de columnas que tuvo en entrenamiento. El preprocessor garantiza que esto pase sin errores."
   ],
   "id": "8c94be9b551ba109"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Componer con ColumnTransformer - Aplicamos cada sub-pipeline en su grupo de columnas.\n",
    "# Esto garantiza que nunca se escalen dummies ni se apliquen transformaciones err√≥neas.\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", num_pipeline, cols_numericas),\n",
    "    (\"cat\", cat_pipeline, cols_categoricas),\n",
    "])\n",
    "\n",
    "preprocessor  # inspecci√≥n r√°pida de la configuraci√≥n"
   ],
   "id": "efd2dc938c83e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Paso 6:** Split Train/Test + Fit/Transform del preprocesador\n",
    "\n",
    "Objetivos:\n",
    "1) Dividir el dataset en **entrenamiento** (80%) y **prueba** (20%) usando estratificaci√≥n.\n",
    "2) Ajustar (`fit`) el `preprocessor` **solo** con `X_train` (evitar data leakage).\n",
    "3) Transformar `X_train` y `X_test` con **las mismas reglas**.\n",
    "4) (Opcional) Reconstruir DataFrames con nombres de columnas para inspecci√≥n.\n"
   ],
   "id": "a699fae9a48d47af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Requiere que ya tengas del Paso 5: X_raw, y, preprocessor, cols_numericas, cols_categoricas\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_raw, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Tama√±os ->\",\n",
    "      \"X_train:\", X_train.shape, \"| X_test:\", X_test.shape,\n",
    "      \"| y_train:\", y_train.shape, \"| y_test:\", y_test.shape)"
   ],
   "id": "1ac8cc698567a584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nProporci√≥n de clases en y_train:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))"
   ],
   "id": "6ed7676001659d62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nProporci√≥n de clases en y_test:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ],
   "id": "86785fb29f8f4d06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Ajuste y transformaci√≥n\n",
    "\n",
    "- `preprocessor.fit(X_train)` **aprende**: medianas (imputaci√≥n), categor√≠as (One-Hot) y medias/desv√≠os (escalado).\n",
    "- Luego aplicamos `transform` a `X_train` y `X_test` con **las mismas reglas** (sin recalcular nada con el test).\n"
   ],
   "id": "72b9e6d68b696cef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ajuste solo con entrenamiento\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Transformaciones consistentes en train y test\n",
    "X_train_proc = preprocessor.transform(X_train)\n",
    "X_test_proc  = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Tipos de salida:\", type(X_train_proc), type(X_test_proc))\n",
    "print(\"Dimensiones ->\",\n",
    "      \"X_train_proc:\", X_train_proc.shape, \"| X_test_proc:\", X_test_proc.shape)\n"
   ],
   "id": "eff7a29e16ad085",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Reconstrucci√≥n de DataFrames (inspecci√≥n)\n",
    "\n",
    "- Concatenamos nombres de columnas num√©ricas y las columnas generadas por One-Hot.\n",
    "- Convertimos a denso **solo para mirar** (no necesario para entrenar)."
   ],
   "id": "13bdede89c838779"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Nombres para el bloque num√©rico (se mantienen)\n",
    "num_features_out = cols_numericas\n",
    "\n",
    "# Nombres que gener√≥ el One-Hot en el bloque categ√≥rico\n",
    "ohe = preprocessor.named_transformers_[\"cat\"][\"onehot\"]\n",
    "cat_features_out = list(ohe.get_feature_names_out(cols_categoricas))\n",
    "\n",
    "all_feature_names = num_features_out + cat_features_out\n",
    "\n",
    "# Convertir a denso si es sparse (solo inspecci√≥n)\n",
    "to_dense = lambda A: A.toarray() if hasattr(A, \"toarray\") else A\n",
    "X_train_df = pd.DataFrame(to_dense(X_train_proc), columns=all_feature_names, index=X_train.index)\n",
    "X_test_df  = pd.DataFrame(to_dense(X_test_proc),  columns=all_feature_names, index=X_test.index)\n",
    "\n",
    "print(\"Preview de X_train_df:\")\n",
    "X_train_df.head()"
   ],
   "id": "411742759521f7e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chequeo de seguridad\n",
    "\n",
    "- Verificar que no hay **NaNs** tras la imputaci√≥n.\n",
    "- Confirmar que **train** y **test** comparten el **mismo espacio de caracter√≠sticas** (mismo n√∫mero y orden de columnas).\n",
    "- Todo debe ser **num√©rico** despu√©s del One-Hot."
   ],
   "id": "69c9ad66a969c35b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"NaNs en X_train_df:\", int(X_train_df.isna().sum().sum()))\n",
    "print(\"NaNs en X_test_df:\",  int(X_test_df.isna().sum().sum()))\n",
    "\n",
    "print(\"\\nTipos en X_train_df:\")\n",
    "print(X_train_df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n¬øMismas columnas y orden en train/test?:\",\n",
    "      list(X_train_df.columns) == list(X_test_df.columns))"
   ],
   "id": "3523d273003cbcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Categor√≠as desconocidas en test (opcional)\n",
    "\n",
    "Si `X_test` trae categor√≠as **no vistas** en `X_train`, con `handle_unknown='ignore'`:\n",
    "- No falla: esa(s) categor√≠a(s) se codifican como **todo ceros** en el bloque One-Hot.\n",
    "- Puedes auditar qu√© categor√≠as nuevas aparecieron."
   ],
   "id": "f837357f7bb3da22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "desconocidas_total = {}\n",
    "for col, cats_train in zip(cols_categoricas, ohe.categories_):\n",
    "    cats_test = X_test[col].astype(str).unique()\n",
    "    desconocidas = set(cats_test) - set(cats_train)\n",
    "    if desconocidas:\n",
    "        desconocidas_total[col] = desconocidas\n",
    "\n",
    "if desconocidas_total:\n",
    "    print(\"Categor√≠as desconocidas detectadas en X_test:\")\n",
    "    for c, vals in desconocidas_total.items():\n",
    "        print(f\" - {c}: {vals}\")\n",
    "else:\n",
    "    print(\"No se detectaron categor√≠as desconocidas en X_test.\")"
   ],
   "id": "b8d4e36023019104",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Paso 7:** Aprendizaje de M√°quina (Machine Learning AI)\n",
    "\n",
    "Entrenar un modelo de Machine Learning (en nuestro caso, Regresi√≥n Log√≠stica como l√≠nea base) usando datos crudos, pero dejando que el preprocessor haga todo el trabajo de transformaci√≥n dentro de un √∫nico pipeline.\n",
    "\n",
    "As√≠ logramos:\n",
    "\n",
    "- Consistencia: el modelo siempre recibe datos procesados de la misma manera.\n",
    "- Reproducibilidad: puedes guardar el pipeline completo y reutilizarlo.\n",
    "- Facilidad: no hay que preocuparse por procesar manualmente datos nuevos."
   ],
   "id": "b9d9e74c98123985"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Uso de regresi√≥n log√≠stica\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clasificador = LogisticRegression(max_iter=1000)  # margen de iteraciones para asegurar convergencia\n",
    "# En este contexto, asegurar convergencia con max_iter=1000 significa darle al algoritmo de entrenamiento suficientes intentos (iteraciones) para que encuentre los valores √≥ptimos de los par√°metros del modelo.\n",
    "clasificador"
   ],
   "id": "d4939344a80bc02b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Pipeline final (preprocesamiento + modelo)\n",
    "\n",
    "Ventajas:\n",
    "- Encapsula todo en un solo objeto.\n",
    "- Aplica **exactamente** las mismas transformaciones a cualquier dato nuevo.\n",
    "- Evita errores de orden/columnas y *data leakage*."
   ],
   "id": "fc5f9bca547b9446"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "modelo_log = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),   # imputaci√≥n + one-hot + escalado\n",
    "    (\"classifier\", clasificador)      # regresi√≥n log√≠stica\n",
    "])\n",
    "\n",
    "modelo_log"
   ],
   "id": "61ef943bd8038ed2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Entrenamiento**\n",
    "\n",
    "Entrenamos el pipeline con `X_train`/`y_train`.\n",
    "Internamente:\n",
    "1) `preprocessor.fit_transform(X_train)`\n",
    "2) `classifier.fit(X_train_transformado, y_train)`"
   ],
   "id": "77a27ad4a8bd6318"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "modelo_log.fit(X_train, y_train)",
   "id": "2e0e939b3f3e3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Evaluaci√≥n inicial**\n",
    "\n",
    "M√©tricas:\n",
    "- **Accuracy**: proporci√≥n de aciertos.\n",
    "- **Matriz de confusi√≥n**: errores por clase (FP/FN)."
   ],
   "id": "eece12c3c43f1bc4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Predicci√≥n de clases (umbral 0.5)\n",
    "y_pred = modelo_log.predict(X_test)\n",
    "\n",
    "# M√©tricas clave\n",
    "acc   = accuracy_score(y_test, y_pred)\n",
    "prec  = precision_score(y_test, y_pred, pos_label=1)\n",
    "rec   = recall_score(y_test, y_pred, pos_label=1)\n",
    "f1    = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "# Matriz de confusi√≥n + especificidad\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "specificity = TN / (TN + FP) if (TN + FP) else 0.0\n",
    "\n",
    "print(f\"Accuracy: {acc:.3f} | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f} | Especificidad: {specificity:.3f}\")\n",
    "\n",
    "# Gr√°fico en espa√±ol\n",
    "labels_es = [\"No Beca\", \"S√≠ Beca\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels_es)\n",
    "ax = disp.plot(values_format=\"d\")\n",
    "ax.ax_.set_title(\"Matriz de Confusi√≥n ‚Äî Asignaci√≥n de Becas\")\n",
    "ax.ax_.set_xlabel(\"Etiqueta predicha\"); ax.ax_.set_ylabel(\"Etiqueta real\");"
   ],
   "id": "10d31d584b7c2128",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üìå Significado de cada celda del Gr√°fico - Matriz de Confusi√≥n\n",
    "\n",
    "- TN (True Negative) = 7\n",
    "El modelo acert√≥ diciendo \"No Beca\" en 7 casos donde efectivamente no hab√≠a beca.\n",
    "\n",
    "- FP (False Positive) = 0\n",
    "No hubo casos donde el modelo haya dicho \"S√≠ Beca\" pero en realidad no correspond√≠a.\n",
    "\n",
    "- FN (False Negative) = 2\n",
    "El modelo dijo \"No Beca\" en 2 casos donde s√≠ correspond√≠a beca.\n",
    "**‚ö†Ô∏è Esto significa que neg√≥ el beneficio a 2 alumnos que s√≠ lo merec√≠an.**\n",
    "\n",
    "- TP (True Positive) = 1\n",
    "El modelo acert√≥ en 1 caso diciendo \"S√≠ Beca\" y efectivamente correspond√≠a."
   ],
   "id": "1efa5bb6eff03243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**M√©tricas adicionales**\n",
    "\n",
    "- **classification_report**: precisi√≥n, recall y F1 por clase.\n",
    "- **ROC AUC**: capacidad discriminativa usando probabilidades."
   ],
   "id": "5f6aa9a3e00c90f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Probabilidades de clase positiva\n",
    "y_prob = modelo_log.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# AUC ROC y curva\n",
    "auc_roc = roc_auc_score(y_test, y_prob)\n",
    "fpr, tpr, thr = roc_curve(y_test, y_prob)\n",
    "\n",
    "print(f\"AUC ROC: {auc_roc:.3f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"ROC (AUC={auc_roc:.3f})\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"Tasa de falsos positivos (1 - Especificidad)\")\n",
    "plt.ylabel(\"Tasa de verdaderos positivos (Recall)\")\n",
    "plt.title(\"Curva ROC ‚Äî Asignaci√≥n de Becas\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ],
   "id": "8380b129b4dcb55c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "ap = average_precision_score(y_test, y_prob)  # PR-AUC aproximada\n",
    "\n",
    "print(f\"PR-AUC (Average Precision): {ap:.3f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recalls, precisions, label=f\"PR (AP={ap:.3f})\")\n",
    "plt.xlabel(\"Recall (Sensibilidad)\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Curva Precision‚ÄìRecall ‚Äî Asignaci√≥n de Becas\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ],
   "id": "af0cade18806a5b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def resumen_umbral(y_true, y_score, thresholds):\n",
    "    filas = []\n",
    "    for t in thresholds:\n",
    "        y_hat = (y_score >= t).astype(int)\n",
    "        prec = precision_score(y_true, y_hat, zero_division=0)\n",
    "        rec  = recall_score(y_true, y_hat, zero_division=0)\n",
    "        f1   = f1_score(y_true, y_hat, zero_division=0)\n",
    "        TN, FP, FN, TP = confusion_matrix(y_true, y_hat, labels=[0,1]).ravel()\n",
    "        filas.append({\"umbral\": t, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"FP\": FP, \"FN\": FN, \"TP\": TP, \"TN\": TN})\n",
    "    return pd.DataFrame(filas).sort_values(\"umbral\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "umbrales = np.linspace(0.9, 0.1, 9)  # 0.9 ‚Üí 0.1\n",
    "tabla_umbral = resumen_umbral(y_test, y_prob, umbrales)\n",
    "tabla_umbral"
   ],
   "id": "71382d9340a9cfda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = {\n",
    "    \"accuracy\": \"accuracy\",\n",
    "    \"precision\": \"precision\",\n",
    "    \"recall\": \"recall\",\n",
    "    \"f1\": \"f1\",\n",
    "    \"roc_auc\": \"roc_auc\",\n",
    "    \"pr_auc\": \"average_precision\"\n",
    "}\n",
    "\n",
    "cv_res = cross_validate(modelo_log, X_raw, y, cv=cv, scoring=scoring, n_jobs=-1, return_train_score=False)\n",
    "pd.DataFrame(cv_res).agg(['mean','std']).T.round(3)"
   ],
   "id": "b66435d4d6f12d3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### **Paso 8:** Prueba del modelo de ML con nuevos datos\n",
    "\n",
    "Ac√° haremos una prueba del modelo con datos de alumnos nuevos que no tienen becas asignadas."
   ],
   "id": "1f91eeccee13dfe8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "nuevos_postulantes = pd.DataFrame([\n",
    "    {\n",
    "        \"edad\": 18,\n",
    "        \"puntaje_paes\": 640,\n",
    "        \"promedio_notas\": 5.6,\n",
    "        \"ingreso_familiar\": 820000,\n",
    "        \"sexo\": \"F\",\n",
    "        \"tipo_colegio\": \"subvencionado\",\n",
    "        \"comuna\": \"La Florida\"\n",
    "    },\n",
    "    {\n",
    "        \"edad\": 19,\n",
    "        \"puntaje_paes\": 705,\n",
    "        \"promedio_notas\": 6.1,\n",
    "        \"ingreso_familiar\": 130000,\n",
    "        \"sexo\": \"M\",\n",
    "        \"tipo_colegio\": \"municipal\",\n",
    "        \"comuna\": \"Puente Alto\"\n",
    "    },\n",
    "    {\n",
    "        \"edad\": 18,\n",
    "        \"puntaje_paes\": 610,\n",
    "        \"promedio_notas\": 5.2,\n",
    "        \"ingreso_familiar\": 780000,\n",
    "        \"sexo\": \"F\",\n",
    "        \"tipo_colegio\": \"particular\",\n",
    "        \"comuna\": \"√ëu√±oa\"\n",
    "    },\n",
    "    {\n",
    "        \"edad\": 17,\n",
    "        \"puntaje_paes\": 680,\n",
    "        \"promedio_notas\": 5.8,\n",
    "        \"ingreso_familiar\": 890000,\n",
    "        \"sexo\": \"M\",\n",
    "        \"tipo_colegio\": \"subvencionado\",\n",
    "        \"comuna\": \"Maip√∫\"\n",
    "    }\n",
    "])\n",
    "\n",
    "nuevos_postulantes.head()"
   ],
   "id": "bcd0ef2fe3083ad9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Predicci√≥n de clases (True/False)\n",
    "pred_clase = modelo_log.predict(nuevos_postulantes)\n",
    "\n",
    "# Probabilidad de clase positiva (S√≠ Beca = 1)\n",
    "pred_proba = modelo_log.predict_proba(nuevos_postulantes)[:, 1]\n",
    "\n",
    "# Resultado consolidado\n",
    "resultado_nuevos = nuevos_postulantes.copy()\n",
    "resultado_nuevos[\"prediccion_beca\"] = pred_clase\n",
    "resultado_nuevos[\"prob_s√≠_beca\"] = pred_proba.round(3)\n",
    "\n",
    "resultado_nuevos"
   ],
   "id": "1f53d6fb973c5f4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- `prediccion_beca`: `True` si el modelo clasifica como **S√≠ Beca**, `False` si **No Beca**.\n",
    "- `prob_s√≠_beca`: qu√© tan seguro est√° el modelo de asignar beca (umbral por defecto = 0.5)."
   ],
   "id": "3094cd8c0edec544"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
